{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Asakeblessing/Data-Science-Project/blob/main/FinalSupplier_Performance_Data_Science_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data PreProcessing"
      ],
      "metadata": {
        "id": "jyPe28ewSaun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries Import\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "jWjddoO4lM8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aD4sDYR0SWJl"
      },
      "outputs": [],
      "source": [
        "#Install Packages\n",
        "\n",
        "!pip install pandas numpy scikit-learn matplotlib seaborn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BVfgirppp2A-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load my Dataset\n",
        "\n",
        "df = pd.read_csv(\"Supplier selection and performance Dataset.csv\")\n",
        "\n",
        "#show the first few rows\n",
        "df.head()"
      ],
      "metadata": {
        "id": "qScl5JKfS6YM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleaning"
      ],
      "metadata": {
        "id": "TNRY9iIJTp2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to datetime\n",
        "df['Order_Date'] = pd.to_datetime(df['Order_Date'])\n",
        "df['Delivery_Date'] = pd.to_datetime(df['Delivery_Date'])\n",
        "\n",
        "# Fill missing Delivery_Date based on average delivery duration\n",
        "avg_delivery_days = (df['Delivery_Date'] - df['Order_Date']).dt.days.mean()\n",
        "df['Delivery_Date'] = df['Delivery_Date'].fillna(df['Order_Date'] + pd.to_timedelta(avg_delivery_days, unit='D'))\n",
        "\n",
        "# Fill missing Defective_Units with mean\n",
        "df['Defective_Units'] = df['Defective_Units'].fillna(df['Defective_Units'].mean())\n",
        "\n",
        "df.info()\n",
        "df.isnull().sum()\n"
      ],
      "metadata": {
        "id": "vHmQcA1JTxnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop_duplicates()\n"
      ],
      "metadata": {
        "id": "XuaYleg0VKh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KPI Engineering"
      ],
      "metadata": {
        "id": "TAHcaIxvVl4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Cost_Savings'] = (df['Unit_Price'] - df['Negotiated_Price']) * df['Quantity']\n",
        "df['Delivery_Duration'] = (df['Delivery_Date'] - df['Order_Date']).dt.days\n",
        "df['Defect_Rate_Percent'] = (df['Defective_Units'] / df['Quantity']) * 100\n"
      ],
      "metadata": {
        "id": "9AHq3LpJWjLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Prep For Topsis"
      ],
      "metadata": {
        "id": "cH-SBasyWoy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topsis_df = df[['Supplier', 'Cost_Savings', 'Delivery_Duration', 'Defect_Rate_Percent']]\n",
        "topsis_grouped = topsis_df.groupby('Supplier').mean().reset_index()\n"
      ],
      "metadata": {
        "id": "_xTJvG0zWr3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalize and Weight the KPIs"
      ],
      "metadata": {
        "id": "ACtnkFWbWzMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Normalize the KPIs using Min-Max Scaling\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "normalized = scaler.fit_transform(topsis_grouped[['Cost_Savings', 'Delivery_Duration', 'Defect_Rate_Percent']])\n",
        "\n",
        "weights = np.array([0.4, 0.3, 0.3])  # Set weights for the KPIs\n",
        "impacts = np.array([1, -1, -1])      # +1 for beneficial, -1 for non-beneficial\n",
        "\n",
        "# Weighted normalized matrix\n",
        "weighted = normalized * weights\n",
        "\n"
      ],
      "metadata": {
        "id": "7Hztuv-3W7P8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ideal and Negative-Ideal Solutions"
      ],
      "metadata": {
        "id": "qqp4K9aRYKBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ideal = np.max(weighted, axis=0) * (impacts > 0) + np.min(weighted, axis=0) * (impacts < 0)\n",
        "negative_ideal = np.min(weighted, axis=0) * (impacts > 0) + np.max(weighted, axis=0) * (impacts < 0)\n",
        "\n"
      ],
      "metadata": {
        "id": "e-pv1RriXK_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Euclidean distances Calculation"
      ],
      "metadata": {
        "id": "8TnyZuSmYcWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dist_to_ideal = np.linalg.norm(weighted - ideal, axis=1)\n",
        "dist_to_negative_ideal = np.linalg.norm(weighted - negative_ideal, axis=1)\n"
      ],
      "metadata": {
        "id": "gPlwoeifYW6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOPSIS Score"
      ],
      "metadata": {
        "id": "KKoa9YMPYwLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topsis_scores = dist_to_negative_ideal / (dist_to_ideal + dist_to_negative_ideal)\n"
      ],
      "metadata": {
        "id": "cd2TdaL1Ysfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ranking For TOPSIS Score"
      ],
      "metadata": {
        "id": "DpyTxnc-ZIgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topsis_grouped['TOPSIS_Score'] = topsis_scores\n",
        "topsis_grouped['Rank'] = topsis_grouped['TOPSIS_Score'].rank(ascending=False)\n",
        "topsis_ranked = topsis_grouped.sort_values(by='Rank')\n"
      ],
      "metadata": {
        "id": "42BeSOOHZZtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualize TOPSIS Score using Barchart\n",
        "\n",
        "topsis_ranked_sorted = topsis_ranked.sort_values(by='TOPSIS_Score', ascending=True)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(\n",
        "    topsis_ranked_sorted['Supplier'],\n",
        "    topsis_ranked_sorted['TOPSIS_Score'],\n",
        "    color='mediumseagreen'\n",
        ")\n",
        "plt.xlabel('TOPSIS Score')\n",
        "plt.title('Supplier Ranking Based on TOPSIS Score')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WgUshhQkagq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Final Ranking\n",
        "\n",
        "topsis_ranked.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "3Cih2WSna-bD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign supplier performance class based on TOPSIS Score tertiles\n",
        "topsis_ranked['Performance_Class'] = pd.qcut(\n",
        "    topsis_ranked['TOPSIS_Score'],\n",
        "    q=3,\n",
        "    labels=['Low', 'Medium', 'High']\n",
        ")\n"
      ],
      "metadata": {
        "id": "7FNzt8D4b0Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pie chart\n",
        "class_counts = topsis_ranked['Performance_Class'].value_counts() # Calculate class counts here\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.pie(class_counts.values, labels=class_counts.index, autopct='%1.1f%%', colors=['lightcoral', 'gold', 'mediumseagreen'])\n",
        "plt.title('Supplier Performance Class Distribution')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "seXa_2w0bprA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create grouped table\n",
        "grouped_suppliers = topsis_ranked[['Supplier', 'Performance_Class']].sort_values(by='Performance_Class')\n",
        "grouped_suppliers\n"
      ],
      "metadata": {
        "id": "chikX2zJciox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topsis_ranked.to_csv(\"TOPSIS_final_ranking.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "WdoS8PaBYgM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine Learning Pipepline Using All 3 KPIs for Performance Class/ Independent Of Topsis"
      ],
      "metadata": {
        "id": "HjU9EqBChnVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame for ML\n",
        "kpi_df = df[['Supplier', 'Cost_Savings', 'Delivery_Duration', 'Defect_Rate_Percent']].copy()\n"
      ],
      "metadata": {
        "id": "_Tr1xnGwu5Z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing or invalid KPI values\n",
        "kpi_df.dropna(subset=['Cost_Savings', 'Delivery_Duration', 'Defect_Rate_Percent'], inplace=True)\n"
      ],
      "metadata": {
        "id": "_zE_7qr2vQyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Binary Performance label using all 3 KPIs Cost_Savings,Delivery_Duration and Defect_Rate_Percent"
      ],
      "metadata": {
        "id": "4ihDNc4orOj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define binary performance label\n",
        "def classify_binary(row):\n",
        "    return 1 if (\n",
        "        row['Cost_Savings'] > 3500 and\n",
        "        row['Delivery_Duration'] < 8 and\n",
        "        row['Defect_Rate_Percent'] < 7\n",
        "    ) else 0\n",
        "\n",
        "# Apply it\n",
        "kpi_df['Performance_Binary'] = kpi_df.apply(classify_binary, axis=1)\n"
      ],
      "metadata": {
        "id": "8-qCTgNO44Dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(kpi_df.columns)"
      ],
      "metadata": {
        "id": "5V-b4FRTNnd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kpi_df[['Cost_Savings', 'Delivery_Duration', 'Defect_Rate_Percent', 'Performance_Binary']].head()\n"
      ],
      "metadata": {
        "id": "qrUCL1Uj5oMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features and Binary Labels"
      ],
      "metadata": {
        "id": "ADERwdw7-zXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = kpi_df[['Cost_Savings', 'Delivery_Duration', 'Defect_Rate_Percent']]\n",
        "y = kpi_df['Performance_Binary']"
      ],
      "metadata": {
        "id": "J8USBQk4-yMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train/Test Split"
      ],
      "metadata": {
        "id": "z9rQkbxN7T9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "7shjGQTs_GVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Regularized Models With Class Balancing"
      ],
      "metadata": {
        "id": "0qe1UcjWDRLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from collections import Counter\n",
        "\n",
        "# Calculate ratio for XGBoost class weight\n",
        "class_counts = Counter(y)\n",
        "scale_ratio = class_counts[0] / class_counts[1]\n"
      ],
      "metadata": {
        "id": "nZOZ_FooDaQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ML Model Training with Regularization and Class Weights"
      ],
      "metadata": {
        "id": "cHKbSobGs45z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_model = LogisticRegression(\n",
        "    class_weight='balanced',\n",
        "    C=0.05,  # Stronger regularization\n",
        "    penalty='l2',\n",
        "    solver='liblinear',\n",
        "    max_iter=1000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=30,             # Fewer trees to reduce overfitting\n",
        "    max_depth=2,                 # Very shallow trees (cannot memorize)\n",
        "    min_samples_leaf=10,         # Prevent learning from noise\n",
        "    min_samples_split=10,\n",
        "    max_features='sqrt',         # O use sqrt(n_features) at each split\n",
        "    bootstrap=True,              # to Ensure sampling randomness\n",
        "    class_weight='balanced',     # helps Handle class imbalance\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgb_model = XGBClassifier(\n",
        "    n_estimators=50,            #Balanced number of boosting rounds to learn patters without over fittting\n",
        "    max_depth=2,                #shallow trees to avoid memorising noise : focus on simple and general rules\n",
        "    learning_rate=0.01,         #small learning rate to improve stabiility and require more boosting rounds for gradual learning\n",
        "    subsample=0.6,              #small to increase diversity\n",
        "    colsample_bytree=0.6,       #reduce overfitting\n",
        "    reg_lambda=10,             # L2 regularization\n",
        "    reg_alpha=5,               # L1 regularization\n",
        "    scale_pos_weight=10,       # Balance class distribution\n",
        "    eval_metric='logloss',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "# Model dictionary\n",
        "models = {\n",
        "    \"Logistic Regression\": logistic_model,\n",
        "    \"Random Forest\": rf_model,\n",
        "    \"XGBoost\": xgb_model\n",
        "}"
      ],
      "metadata": {
        "id": "sL5MG4aADf_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate and Cross - Validation"
      ],
      "metadata": {
        "id": "gO-JKIUw7poF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "scores = cross_val_score(model, X, y, scoring='f1_macro', cv=skf)\n",
        "print(\"Stratified 5-Fold F1 Macro:\", scores.mean())\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n {name} Evaluation\")\n",
        "\n",
        "    # Train  model\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Confusion Matrix\n",
        "    print(\" Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    # Classification Report\n",
        "    print(\"\\n Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['Low', 'High']))\n",
        "\n",
        "    # Cross-validation F1 macro\n",
        "    scores = cross_val_score(model, X, y, cv=skf, scoring='f1_macro')\n",
        "    print(f\" Stratified 5-Fold F1 Macro Avg: {scores.mean():.4f}\")\n"
      ],
      "metadata": {
        "id": "V7Ms2pN7DmVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion Matrices"
      ],
      "metadata": {
        "id": "-v6ldrbW_03g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "#  confusion matrices from evaluations\n",
        "cm_logistic = np.array([[128, 14],\n",
        "                        [1, 13]])\n",
        "\n",
        "cm_rf = np.array([[134, 8],\n",
        "                  [0, 14]])\n",
        "\n",
        "cm_xgb = np.array([[136, 6],\n",
        "                   [0, 14]])\n",
        "\n",
        "# Model titles\n",
        "model_titles = ['Logistic Regression', 'Random Forest', 'XGBoost']\n",
        "cms = [cm_logistic, cm_rf, cm_xgb]\n",
        "\n",
        "# Plot confusion matrices side-by-side\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "for ax, cm, title in zip(axes, cms, model_titles):\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Low', 'High'])\n",
        "    disp.plot(ax=ax, cmap='Blues', colorbar=False)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel('Predicted')\n",
        "    ax.set_ylabel('Actual')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yMCJTo2DbrB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best Performing Models"
      ],
      "metadata": {
        "id": "ToKklLZZGvoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#   model scores  evaluation\n",
        "model_scores = {\n",
        "    'Logistic Regression': 0.7382,\n",
        "    'Random Forest': 0.8391,\n",
        "    'XGBoost': 0.8535\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "score_df = pd.DataFrame(list(model_scores.items()), columns=['Model', 'F1 Macro Score'])\n",
        "\n",
        "# Plot F1 Macro Scores\n",
        "plt.figure(figsize=(9, 5))\n",
        "bars = plt.bar(score_df['Model'], score_df['F1 Macro Score'], color=['#4c78a8', '#72b7b2', '#f58518'])\n",
        "plt.title('Model Performance Comparison (F1 Macro Score)', fontsize=14)\n",
        "\n",
        "plt.ylim(0, 1)\n",
        "plt.ylabel('F1 Macro Score')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "\n",
        "# Annotate bars with values\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.01, f'{yval:.4f}', ha='center', fontsize=11)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xCyq9SjtcRVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save your final DataFrame (e.g., kpi_df) to CSV\n",
        "kpi_df.to_csv(\"final_supplier_results.csv\", index=False)\n",
        "\n",
        "# Provide download link\n",
        "from google.colab import files\n",
        "files.download(\"final_supplier_results.csv\")"
      ],
      "metadata": {
        "id": "sR-J_ODDUTeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supplier Predictions across Models"
      ],
      "metadata": {
        "id": "wuKZP8fTyWYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, joblib\n",
        "import pandas as pd\n",
        "\n",
        "# Configuration\n",
        "FEATURES = [\"Cost_Savings\", \"Delivery_Duration\", \"Defect_Rate_Percent\"]\n",
        "THRESHOLDS = {\"LogisticRegression\":0.5, \"RandomForest\":0.5, \"XGBoost\":0.5}\n",
        "PKL_PATHS = {\"LogisticRegression\":\"lr_model.pkl\", \"RandomForest\":\"rf_model.pkl\", \"XGBoost\":\"xgb_model.pkl\"}\n",
        "\n",
        "#recal\n",
        "if \"df_all\" not in globals():\n",
        "    df_all = pd.DataFrame({\n",
        "        \"Supplier\":[\"Alpha_Inc\",\"Beta_Supplies\",\"Gamma_Co\",\"Delta_Logistics\",\"Epsilon_Group\"],\n",
        "        \"Cost_Savings\":[4000,3600,3100,2900,3700],\n",
        "        \"Delivery_Duration\":[7,6,5,10,9],\n",
        "        \"Defect_Rate_Percent\":[5.5,4.2,3.8,8.1,6.0]\n",
        "    })\n",
        "\n",
        "#  checks/casting\n",
        "req = [\"Supplier\"] + FEATURES\n",
        "missing = [c for c in req if c not in df_all.columns]\n",
        "if missing: raise ValueError(f\"df_all must contain: {missing}\")\n",
        "df_all[FEATURES] = df_all[FEATURES].apply(pd.to_numeric, errors=\"coerce\").fillna(df_all[FEATURES].median(numeric_only=True))\n",
        "\n",
        "# Models:\n",
        "try:\n",
        "    models  # noqa\n",
        "    print(\"Using models from memory.\")\n",
        "except NameError:\n",
        "    print(\"Loading models from disk...\")\n",
        "    models = {name: joblib.load(path) for name, path in PKL_PATHS.items() if os.path.exists(path)}\n",
        "    if not models: raise RuntimeError(\"No models provided and no .pkl files found.\")\n",
        "for n, m in models.items():\n",
        "    if not hasattr(m, \"predict_proba\"): raise AttributeError(f\"{n} lacks predict_proba().\")\n",
        "\n",
        "#  Row-level scoring\n",
        "def score_rows(df, model_dict, feats=FEATURES, thr=THRESHOLDS):\n",
        "    X = df[feats].copy()\n",
        "    out = []\n",
        "    for name, model in model_dict.items():\n",
        "        p = model.predict_proba(X)[:, 1]\n",
        "        out.append(pd.DataFrame({\"Supplier\": df[\"Supplier\"], \"Model\": name, \"Prob_High\": p, \"Pred\": (p >= thr.get(name,0.5)).astype(int)}))\n",
        "    return pd.concat(out, ignore_index=True)\n",
        "\n",
        "row_preds = score_rows(df_all, models)\n",
        "\n",
        "# Supplier aggregation & ranking (unchanged logic)\n",
        "sup_agg = (row_preds.groupby([\"Model\",\"Supplier\"], as_index=False)\n",
        "           .agg(Prob_High_mean=(\"Prob_High\",\"mean\"), High_rate=(\"Pred\",\"mean\")))\n",
        "sup_agg[\"Model_Rank\"] = (sup_agg.groupby(\"Model\")[\"Prob_High_mean\"]\n",
        "                         .rank(ascending=False, method=\"dense\").astype(int))\n",
        "\n",
        "rank_wide = sup_agg.pivot(index=\"Supplier\", columns=\"Model\", values=\"Model_Rank\").rename_axis(None, axis=1).reset_index()\n",
        "prob_wide = sup_agg.pivot(index=\"Supplier\", columns=\"Model\", values=\"Prob_High_mean\").rename_axis(None, axis=1).reset_index()\n",
        "comparison = rank_wide.merge(prob_wide, on=\"Supplier\", suffixes=(\"_Rank\",\"_Prob\"))\n",
        "\n",
        "print(\"\\n=== Supplier ranking & mean probability by model (no TOPSIS) ===\")\n",
        "print(comparison.to_string(index=False))\n",
        "\n",
        "comparison.to_csv(\"supplier_model_comparison_no_topsis.csv\", index=False)\n",
        "sup_agg.to_csv(\"supplier_model_scores_long_no_topsis.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "j9E0XDLJwon2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Visualize ranks across models\n",
        "\n",
        "rank_cols = [c for c in comparison.columns if c.endswith(\"_Rank\")]\n",
        "if rank_cols:\n",
        "    ax = comparison.set_index(\"Supplier\")[rank_cols].plot(kind=\"bar\", figsize=(12,6))\n",
        "    ax.set_ylabel(\"Rank (1 = best)\")\n",
        "    ax.set_title(\"Supplier Ranks by Model (LogReg, RF, XGB)\")\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No rank columns to plot.\")"
      ],
      "metadata": {
        "id": "CCssS_gRvIqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topsis Vs Machine Learning\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DN01blR7LLO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize XGBoost vs TOPSIS ranking\n",
        "# comparison (ML results) and 'topsis_df' (TOPSIS results)\n",
        "\n",
        "# Merge ML comparison with TOPSIS ranks\n",
        "topsis = topsis_grouped[[\"Supplier\", \"Rank\"]].rename(columns={\"Rank\": \"TOPSIS_Rank\"})\n",
        "comp_with_topsis = comparison.merge(topsis, on=\"Supplier\", how=\"inner\")\n",
        "\n",
        "# Plot XGBoost vs TOPSIS rank side by side\n",
        "target_model = \"XGBoost\"\n",
        "if f\"{target_model}_Rank\" in comp_with_topsis.columns:\n",
        "    plt.figure(figsize=(12,6))\n",
        "    idx = np.arange(len(comp_with_topsis))\n",
        "    bar_w = 0.4\n",
        "    plt.bar(idx, comp_with_topsis[f\"{target_model}_Rank\"], bar_w, label=f'{target_model} Rank')\n",
        "    plt.bar(idx+bar_w, comp_with_topsis[\"TOPSIS_Rank\"], bar_w, label='TOPSIS Rank')\n",
        "    plt.xticks(idx+bar_w/2, comp_with_topsis[\"Supplier\"], rotation=45, ha=\"right\")\n",
        "    plt.ylabel(\"Rank (1 = best)\")\n",
        "    plt.title(f\"Supplier Rankings: {target_model} vs TOPSIS\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(f\"No {target_model}_Rank column found in comparison table.\")"
      ],
      "metadata": {
        "id": "W1QgFXAqC4dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Powerbi Deployment Pipeline"
      ],
      "metadata": {
        "id": "Dl_rgmGODm0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# File paths\n",
        "MODEL_FILE = \"xgb_model.pkl\"\n",
        "META_FILE  = \"model_meta.json\"\n",
        "# INPUT_FILE = \"suppliers_input.csv\"      # KPI dataset # Removed\n",
        "OUTPUT_FILE = \"suppliers_scored.csv\"    # predictions output\n",
        "\n",
        "#  Load data # Modified to use existing df\n",
        "# df = pd.read_csv(INPUT_FILE) # Removed\n",
        "\n",
        "#  Load model + metadata\n",
        "# Assuming the model and meta files were saved previously\n",
        "try:\n",
        "    model = joblib.load(MODEL_FILE)\n",
        "    with open(META_FILE, \"r\") as f:\n",
        "        meta = json.load(f)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Model or metadata file not found. Please ensure '{MODEL_FILE}' and '{META_FILE}' exist.\")\n",
        "    # Exit or handle the error appropriately\n",
        "    raise # Re-raise the exception to stop execution if files are missing\n",
        "\n",
        "\n",
        "required = [\"Cost_Savings\", \"Delivery_Duration\", \"Defect_Rate_Percent\"]\n",
        "feat_order = meta.get(\"feature_order\", required)\n",
        "thr = meta.get(\"thresholds\", {\"cost_savings\": 3500, \"delivery_duration\": 8, \"defect_rate\": 7})\n",
        "\n",
        "#  numeric\n",
        "for c in required:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "df[required] = df[required].fillna(df[required].median(numeric_only=True))\n",
        "\n",
        "# Prediction\n",
        "X = df[feat_order]\n",
        "df[\"ML_Pred\"] = model.predict(X).astype(int)\n",
        "\n",
        "try:\n",
        "    df[\"Prob_High\"] = model.predict_proba(X)[:, 1]\n",
        "    df[\"Prob_High_Pct\"] = (df[\"Prob_High\"] * 100).round(1)\n",
        "except Exception:\n",
        "    df[\"Prob_High\"] = np.nan\n",
        "    df[\"Prob_High_Pct\"] = np.nan\n",
        "\n",
        "\n",
        "df[\"ML_Pred_Label\"] = np.where(df[\"ML_Pred\"] == 1, \"High\", \"Low\")\n",
        "\n",
        "# Rule-based label for comparison\n",
        "df[\"Rule_Based\"] = (\n",
        "    (df[\"Cost_Savings\"] > thr[\"cost_savings\"]) &\n",
        "    (df[\"Delivery_Duration\"] < thr[\"delivery_duration\"]) &\n",
        "    (df[\"Defect_Rate_Percent\"] < thr[\"defect_rate\"])\n",
        ").astype(int)\n",
        "\n",
        "# Save predictions\n",
        "df.to_csv(OUTPUT_FILE, index=False)\n",
        "print(f\"âœ… Predictions saved to {OUTPUT_FILE}\")"
      ],
      "metadata": {
        "id": "L11Kd0KakXs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"suppliers_scored.csv\")\n"
      ],
      "metadata": {
        "id": "_bhR8Ajsk6aP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}